{
  "generated_at": "2025-09-05T05:07:10.927144+00:00",
  "items": [
    {
      "source": "OpenAI News",
      "title": "Expanding economic opportunity with AI",
      "link": "https://openai.com/index/expanding-economic-opportunity-with-ai",
      "summary": "OpenAI is launching a Jobs Platform and new Certifications to connect workers with jobs, training, and certifications. Learn how we’re expanding economic opportunity and making AI skills more accessible.",
      "category": "Oportunidades",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "OpenAI News",
      "title": "Vijaye Raji to become CTO of Applications with acquisition of Statsig",
      "link": "https://openai.com/index/vijaye-raji-to-become-cto-of-applications-with-acquisition-of-statsig",
      "summary": "Vijaye Raji will step into a new role as CTO of Applications, reporting to CEO of Applications, Fidji Simo, following the acquisition of Statsig.",
      "category": "Otros",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "OpenAI News",
      "title": "Building more helpful ChatGPT experiences for everyone",
      "link": "https://openai.com/index/building-more-helpful-chatgpt-experiences-for-everyone",
      "summary": "We’re partnering with experts, strengthening protections for teens with parental controls, and routing sensitive conversations to reasoning models in ChatGPT.",
      "category": "Otros",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "OpenAI News",
      "title": "Introducing gpt-realtime and Realtime API updates",
      "link": "https://openai.com/index/introducing-gpt-realtime",
      "summary": "We’re releasing a more advanced speech-to-speech model and new API capabilities including MCP server support, image input, and SIP phone calling support.",
      "category": "Modelos",
      "priority": "MEDIA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "OpenAI News",
      "title": "Supporting nonprofit and community innovation",
      "link": "https://openai.com/index/supporting-nonprofit-and-community-innovation",
      "summary": "OpenAI launches a $50M People-First AI Fund to help U.S. nonprofits scale impact with AI. Applications open Sept 8–Oct 8, 2025 for grants in education, healthcare, research, and more.",
      "category": "Investigación",
      "priority": "MEDIA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "Hugging Face - Blog",
      "title": "Welcome EmbeddingGemma, Google's new efficient embedding model",
      "link": "https://huggingface.co/blog/embeddinggemma",
      "summary": "",
      "category": "Modelos",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "Hugging Face - Blog",
      "title": "Make your ZeroGPU Spaces go brrr with PyTorch ahead-of-time compilation",
      "link": "https://huggingface.co/blog/zerogpu-aoti",
      "summary": "",
      "category": "Otros",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "Hugging Face - Blog",
      "title": "Generate Images with Claude and Hugging Face",
      "link": "https://huggingface.co/blog/claude-and-mcp",
      "summary": "",
      "category": "Otros",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "Hugging Face - Blog",
      "title": "From Zero to GPU: A Guide to Building and Scaling Production-Ready CUDA Kernels",
      "link": "https://huggingface.co/blog/kernel-builder",
      "summary": "",
      "category": "Otros",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "Hugging Face - Blog",
      "title": "MCP for Research: How to Connect AI to Research Tools",
      "link": "https://huggingface.co/blog/mcp-for-research",
      "summary": "",
      "category": "Herramientas",
      "priority": "MEDIA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.AI updates on arXiv.org",
      "title": "PG-Agent: An Agent Powered by Page Graph",
      "link": "https://arxiv.org/abs/2509.03536",
      "summary": "arXiv:2509.03536v1 Announce Type: new  Abstract: Graphical User Interface (GUI) agents possess significant commercial and social value, and GUI agents powered by advanced multimodal large language models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI agents usually utilize sequential episodes of multi-step operations across pages as the prior GUI knowledge, which fails to capture the complex transition relationship between pages, making it challenging for the agents to deeply perceive the GUI environment and generalize to new scenarios. Therefore, we design an automated pipeline to transform the sequential episodes into page graphs, which explicitly model the graph structure of the pages that are naturally connected by actions. To fully utilize the page graphs, we further introduce Retrieval-Augmented Generation (RAG) technology to effectively retrieve reliable perception guidelines of GUI from them, and a tailored multi-agent framework PG-Agent with task decomposition strategy is proposed to be injected with the guidelines so that it can generalize to unseen scenarios. Extensive experiments on various benchmarks demonstrate the effectiveness of PG-Agent, even with limited episodes for page graph construction.",
      "category": "Modelos",
      "priority": "MEDIA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.AI updates on arXiv.org",
      "title": "Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models",
      "link": "https://arxiv.org/abs/2509.03548",
      "summary": "arXiv:2509.03548v1 Announce Type: new  Abstract: We investigate partially identifiable queries in a class of causal models. We focus on acyclic Structural Causal Models that are quasi-Markovian (that is, each endogenous variable is connected with at most one exogenous confounder). We look into scenarios where endogenous variables are observed (and a distribution over them is known), while exogenous variables are not fully specified. This leads to a representation that is in essence a Bayesian network where the distribution of root variables is not uniquely determined. In such circumstances, it may not be possible to precisely compute a probability value of interest. We thus study the computation of tight probability bounds, a problem that has been solved by multilinear programming in general, and by linear programming when a single confounded component is intervened upon. We present a new algorithm to simplify the construction of such programs by exploiting input probabilities over endogenous variables. For scenarios with a single intervention, we apply column generation to compute a probability bound through a sequence of auxiliary linear integer programs, thus showing that a representation with polynomial cardinality for exogenous variables is possible. Experiments show column generation techniques to be superior to existing methods.",
      "category": "Productos",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.AI updates on arXiv.org",
      "title": "Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method",
      "link": "https://arxiv.org/abs/2509.03550",
      "summary": "arXiv:2509.03550v1 Announce Type: new  Abstract: In the context of continuously rising global air traffic, efficient and safe Conflict Detection and Resolution (CD&amp;R) is paramount for air traffic management. Although Deep Reinforcement Learning (DRL) offers a promising pathway for CD&amp;R automation, existing approaches commonly suffer from a \"unimodal bias\" in their policies. This leads to a critical lack of decision-making flexibility when confronted with complex and dynamic constraints, often resulting in \"decision deadlocks.\" To overcome this limitation, this paper pioneers the integration of diffusion probabilistic models into the safety-critical task of CD&amp;R, proposing a novel autonomous conflict resolution framework named Diffusion-AC. Diverging from conventional methods that converge to a single optimal solution, our framework models its policy as a reverse denoising process guided by a value function, enabling it to generate a rich, high-quality, and multimodal action distribution. This core architecture is complemented by a Density-Progressive Safety Curriculum (DPSC), a training mechanism that ensures stable and efficient learning as the agent progresses from sparse to high-density traffic environments. Extensive simulation experiments demonstrate that the proposed method significantly outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the most challenging high-density scenarios, Diffusion-AC not only maintains a high success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions (NMACs) by approximately 59% compared to the next-best-performing baseline, significantly enhancing the system's safety margin. This performance leap stems from its unique multimodal decision-making capability, which allows the agent to flexibly switch to effective alternative maneuvers.",
      "category": "Modelos",
      "priority": "MEDIA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.AI updates on arXiv.org",
      "title": "Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents",
      "link": "https://arxiv.org/abs/2509.03581",
      "summary": "arXiv:2509.03581v1 Announce Type: new  Abstract: Training large language models (LLMs) to reason via reinforcement learning (RL) significantly improves their problem-solving capabilities. In agentic settings, existing methods like ReAct prompt LLMs to explicitly plan before every action; however, we demonstrate that always planning is computationally expensive and degrades performance on long-horizon tasks, while never planning further limits performance. To address this, we introduce a conceptual framework formalizing dynamic planning for LLM agents, enabling them to flexibly decide when to allocate test-time compute for planning. We propose a simple two-stage training pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for dynamic planning, and (2) RL to refine this capability in long-horizon environments. Experiments on the Crafter environment show that dynamic planning agents trained with this approach are more sample-efficient and consistently achieve more complex objectives. Additionally, we demonstrate that these agents can be effectively steered by human-written plans, surpassing their independent capabilities. To our knowledge, this work is the first to explore training LLM agents for dynamic test-time compute allocation in sequential decision-making tasks, paving the way for more efficient, adaptive, and controllable agentic systems.",
      "category": "Productos",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.AI updates on arXiv.org",
      "title": "Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE",
      "link": "https://arxiv.org/abs/2509.03626",
      "summary": "arXiv:2509.03626v1 Announce Type: new  Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive progress but still produces hallucinations and unverifiable claims, limiting reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves accuracy by grounding outputs in external knowledge, especially in domains like healthcare, where precision is vital. However, RAG remains opaque and essentially a black box, heavily dependent on data quality. We developed a method-agnostic, perturbation-based framework that provides token and component-level interoperability for Graph RAG using SMILE and named it as Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing similarities, and training weighted linear surrogates, KG-SMILE identifies the graph entities and relations most influential to generated outputs, thereby making RAG more transparent. We evaluate KG-SMILE using comprehensive attribution metrics, including fidelity, faithfulness, consistency, stability, and accuracy. Our findings show that KG-SMILE produces stable, human-aligned explanations, demonstrating its capacity to balance model effectiveness with interpretability and thereby fostering greater transparency and trust in machine learning technologies.",
      "category": "Modelos",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.LG updates on arXiv.org",
      "title": "The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric",
      "link": "https://arxiv.org/abs/2509.03594",
      "summary": "arXiv:2509.03594v1 Announce Type: new  Abstract: We present a class of novel optimisers for training neural networks that makes use of the Riemannian metric naturally induced when the loss landscape is embedded in higher-dimensional space. This is the same metric that underlies common visualisations of loss landscapes. By taking this geometric perspective literally and using the induced metric, we develop a new optimiser and compare it to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of tasks and architectures. Empirically, we conclude that this new class of optimisers is highly effective in low dimensional examples, and provides slight improvement over state-of-the-art methods for training neural networks. These new optimisers have theoretically desirable properties. In particular, the effective learning rate is automatically decreased in regions of high curvature acting as a smoothed out form of gradient clipping. Similarly, one variant of these optimisers can also be viewed as inducing an effective scheduled learning rate and decoupled weight decay is the natural choice from our geometric perspective. The basic method can be used to modify any existing preconditioning method. The new optimiser has a computational complexity comparable to that of Adam.",
      "category": "Productos",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.LG updates on arXiv.org",
      "title": "CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records",
      "link": "https://arxiv.org/abs/2509.03643",
      "summary": "arXiv:2509.03643v1 Announce Type: new  Abstract: Electronic Health Records (EHRs) provide a rich, longitudinal view of patient health and hold significant potential for advancing clinical decision support, risk prediction, and data-driven healthcare research. However, most artificial intelligence (AI) models for EHRs are designed for narrow, single-purpose tasks, limiting their generalizability and utility in real-world settings. Here, we present CEHR-GPT, a general-purpose foundation model for EHR data that unifies three essential capabilities - feature representation, zero-shot prediction, and synthetic data generation - within a single architecture. To support temporal reasoning over clinical sequences, \\cehrgpt{} incorporates a novel time-token-based learning framework that explicitly encodes patients' dynamic timelines into the model structure. CEHR-GPT demonstrates strong performance across all three tasks and generalizes effectively to external datasets through vocabulary expansion and fine-tuning. Its versatility enables rapid model development, cohort discovery, and patient outcome forecasting without the need for task-specific retraining.",
      "category": "Modelos",
      "priority": "MEDIA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.LG updates on arXiv.org",
      "title": "Nonnegative matrix factorization and the principle of the common cause",
      "link": "https://arxiv.org/abs/2509.03652",
      "summary": "arXiv:2509.03652v1 Announce Type: new  Abstract: Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction method. The principle of the common cause (PCC) is a basic methodological approach in probabilistic causality, which seeks an independent mixture model for the joint probability of two dependent random variables. It turns out that these two concepts are closely related. This relationship is explored reciprocally for several datasets of gray-scale images, which are conveniently mapped into probability models. On one hand, PCC provides a predictability tool that leads to a robust estimation of the effective rank of NMF. Unlike other estimates (e.g., those based on the Bayesian Information Criteria), our estimate of the rank is stable against weak noise. We show that NMF implemented around this rank produces features (basis images) that are also stable against noise and against seeds of local optimization, thereby effectively resolving the NMF nonidentifiability problem. On the other hand, NMF provides an interesting possibility of implementing PCC in an approximate way, where larger and positively correlated joint probabilities tend to be explained better via the independent mixture model. We work out a clustering method, where data points with the same common cause are grouped into the same cluster. We also show how NMF can be employed for data denoising.",
      "category": "Modelos",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.LG updates on arXiv.org",
      "title": "Semi-decentralized Federated Time Series Prediction with Client Availability Budgets",
      "link": "https://arxiv.org/abs/2509.03660",
      "summary": "arXiv:2509.03660v1 Announce Type: new  Abstract: Federated learning (FL) effectively promotes collaborative training among distributed clients with privacy considerations in the Internet of Things (IoT) scenarios. Despite of data heterogeneity, FL clients may also be constrained by limited energy and availability budgets. Therefore, effective selection of clients participating in training is of vital importance for the convergence of the global model and the balance of client contributions. In this paper, we discuss the performance impact of client availability with time-series data on federated learning. We set up three different scenarios that affect the availability of time-series data and propose FedDeCAB, a novel, semi-decentralized client selection method applying probabilistic rankings of available clients. When a client is disconnected from the server, FedDeCAB allows obtaining partial model parameters from the nearest neighbor clients for joint optimization, improving the performance of offline models and reducing communication overhead. Experiments based on real-world large-scale taxi and vessel trajectory datasets show that FedDeCAB is effective under highly heterogeneous data distribution, limited communication budget, and dynamic client offline or rejoining.",
      "category": "Modelos",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.LG updates on arXiv.org",
      "title": "AutoGrid AI: Deep Reinforcement Learning Framework for Autonomous Microgrid Management",
      "link": "https://arxiv.org/abs/2509.03666",
      "summary": "arXiv:2509.03666v1 Announce Type: new  Abstract: We present a deep reinforcement learning-based framework for autonomous microgrid management. tailored for remote communities. Using deep reinforcement learning and time-series forecasting models, we optimize microgrid energy dispatch strategies to minimize costs and maximize the utilization of renewable energy sources such as solar and wind. Our approach integrates the transformer architecture for forecasting of renewable generation and a proximal-policy optimization (PPO) agent to make decisions in a simulated environment. Our experimental results demonstrate significant improvements in both energy efficiency and operational resilience when compared to traditional rule-based methods. This work contributes to advancing smart-grid technologies in pursuit of zero-carbon energy systems. We finally provide an open-source framework for simulating several microgrid environments.",
      "category": "Modelos",
      "priority": "MEDIA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.CL updates on arXiv.org",
      "title": "Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies",
      "link": "https://arxiv.org/abs/2509.03525",
      "summary": "arXiv:2509.03525v1 Announce Type: new  Abstract: Over half of US adults with Alzheimer disease and related dementias remain undiagnosed, and speech-based screening offers a scalable detection approach. We compared large language model adaptation strategies for dementia detection using the DementiaBank speech corpus, evaluating nine text-only models and three multimodal audio-text models on recordings from DementiaBank speech corpus. Adaptations included in-context learning with different demonstration selection policies, reasoning-augmented prompting, parameter-efficient fine-tuning, and multimodal integration. Results showed that class-centroid demonstrations achieved the highest in-context learning performance, reasoning improved smaller models, and token-level fine-tuning generally produced the best scores. Adding a classification head substantially improved underperforming models. Among multimodal models, fine-tuned audio-text systems performed well but did not surpass the top text-only models. These findings highlight that model adaptation strategies, including demonstration selection, reasoning design, and tuning method, critically influence speech-based dementia detection, and that properly adapted open-weight models can match or exceed commercial systems.",
      "category": "Modelos",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.CL updates on arXiv.org",
      "title": "Enhancing Speech Large Language Models through Reinforced Behavior Alignment",
      "link": "https://arxiv.org/abs/2509.03526",
      "summary": "arXiv:2509.03526v1 Announce Type: new  Abstract: The recent advancements of Large Language Models (LLMs) have spurred considerable research interest in extending their linguistic capabilities beyond text to other modalities, which leads to emergence of speech-based LLMs (SpeechLMs) with capability of processing user request in either speech or textual formats. However, owing to inter-modal discrepancies, these SpeechLMs still exhibit a significant performance gap compared to their text-based LLM counterparts in instruction-following, particularly when confronted with the dynamic and variable nature of user speech. To address this challenge, this paper introduces a framework termed Reinforced Behavior Alignment (RBA), designed to bolster the language generation proficiency of SpeechLMs. Instead of relying on supervised fine-tuning from human annotations, RBA employs a self-synthesis methodology to generate extensive, high-fidelity alignment data by a powerful teacher LLM. Then SpeechLMs is aligned its behavior with that of a teacher using a reinforcement learning-based approach. Experimental results demonstrate that this method effectively enhances the instruction-following capabilities of SpeechLMs that outperform conventional distillation baselines. Crucially, we demonstrate that RBA can be seamlessly extended to tasks such including spoken question answering and speech-to-text translation, attaining state-of-the-art performance on open benchmarks with only self-generated data.",
      "category": "Productos",
      "priority": "MEDIA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.CL updates on arXiv.org",
      "title": "Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model",
      "link": "https://arxiv.org/abs/2509.03527",
      "summary": "arXiv:2509.03527v1 Announce Type: new  Abstract: In the paper, we consider multilevel multitask analysis of cryptocurrency news using a fine-tuned Mistral 7B large language model with retrieval-augmented generation (RAG).   On the first level of analytics, the fine-tuned model generates graph and text summaries with sentiment scores as well as JSON representations of summaries. Higher levels perform hierarchical stacking that consolidates sets of graph-based and text-based summaries as well as summaries of summaries into comprehensive reports. The combination of graph and text summaries provides complementary views of cryptocurrency news. The model is fine-tuned with 4-bit quantization using the PEFT/LoRA approach. The representation of cryptocurrency news as knowledge graph can essentially eliminate problems with large language model hallucinations.   The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM models for multilevel cryptocurrency news analysis can conduct informative qualitative and quantitative analytics, providing important insights.",
      "category": "Modelos",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.CL updates on arXiv.org",
      "title": "The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process",
      "link": "https://arxiv.org/abs/2509.03528",
      "summary": "arXiv:2509.03528v1 Announce Type: new  Abstract: Process Mining (PM), initially developed for industrial and business contexts, has recently been applied to social systems, including legal ones. However, PM's efficacy in the legal domain is limited by the accessibility and quality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in Italian Chambers), a comprehensive event log of the Italian lawmaking process from 1987 to 2022. Created from unstructured data from the Normattiva portal and structured using large language models (LLMs), ProLiFIC aligns with recent efforts in integrating PM with LLMs. We exemplify preliminary analyses and propose ProLiFIC as a benchmark for legal PM, fostering new developments.",
      "category": "Productos",
      "priority": "MEDIA",
      "impact": "",
      "risks": ""
    },
    {
      "source": "cs.CL updates on arXiv.org",
      "title": "Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages",
      "link": "https://arxiv.org/abs/2509.03529",
      "summary": "arXiv:2509.03529v1 Announce Type: new  Abstract: Earnings calls represent a uniquely rich and semi-structured source of financial communication, blending scripted managerial commentary with unscripted analyst dialogue. Although recent advances in financial sentiment analysis have integrated multi-modal signals, such as textual content and vocal tone, most systems rely on flat document-level or sentence-level models, failing to capture the layered discourse structure of these interactions. This paper introduces a novel multi-modal framework designed to generate semantically rich and structurally aware embeddings of earnings calls, by encoding them as hierarchical discourse trees. Each node, comprising either a monologue or a question-answer pair, is enriched with emotional signals derived from text, audio, and video, as well as structured metadata including coherence scores, topic labels, and answer coverage assessments. A two-stage transformer architecture is proposed: the first encodes multi-modal content and discourse metadata at the node level using contrastive learning, while the second synthesizes a global embedding for the entire conference. Experimental results reveal that the resulting embeddings form stable, semantically meaningful representations that reflect affective tone, structural logic, and thematic alignment. Beyond financial reporting, the proposed system generalizes to other high-stakes unscripted communicative domains such as tele-medicine, education, and political discourse, offering a robust and explainable approach to multi-modal discourse representation. This approach offers practical utility for downstream tasks such as financial forecasting and discourse evaluation, while also providing a generalizable method applicable to other domains involving high-stakes communication.",
      "category": "Modelos",
      "priority": "BAJA",
      "impact": "",
      "risks": ""
    }
  ]
}